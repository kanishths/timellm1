{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a03e96fe-db41-4c07-8216-b051aa5c9cb7",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[43mNote: you may need to restart the kernel using dbutils.library.restartPython() to use updated packages.\u001B[0m\nCollecting bitsandbytes\n  Downloading bitsandbytes-0.43.0-py3-none-manylinux_2_24_x86_64.whl (102.2 MB)\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 102.2/102.2 MB 9.1 MB/s eta 0:00:00\nRequirement already satisfied: torch in /databricks/python3/lib/python3.10/site-packages (from bitsandbytes) (2.0.1+cpu)\nRequirement already satisfied: numpy in /databricks/python3/lib/python3.10/site-packages (from bitsandbytes) (1.23.5)\nRequirement already satisfied: sympy in /databricks/python3/lib/python3.10/site-packages (from torch->bitsandbytes) (1.11.1)\nRequirement already satisfied: typing-extensions in /databricks/python3/lib/python3.10/site-packages (from torch->bitsandbytes) (4.4.0)\nRequirement already satisfied: networkx in /databricks/python3/lib/python3.10/site-packages (from torch->bitsandbytes) (2.8.4)\nRequirement already satisfied: filelock in /databricks/python3/lib/python3.10/site-packages (from torch->bitsandbytes) (3.9.0)\nRequirement already satisfied: jinja2 in /databricks/python3/lib/python3.10/site-packages (from torch->bitsandbytes) (3.1.2)\nRequirement already satisfied: MarkupSafe>=2.0 in /databricks/python3/lib/python3.10/site-packages (from jinja2->torch->bitsandbytes) (2.1.1)\nRequirement already satisfied: mpmath>=0.19 in /databricks/python3/lib/python3.10/site-packages (from sympy->torch->bitsandbytes) (1.2.1)\nInstalling collected packages: bitsandbytes\nSuccessfully installed bitsandbytes-0.43.0\n\u001B[43mNote: you may need to restart the kernel using dbutils.library.restartPython() to use updated packages.\u001B[0m\n"
     ]
    }
   ],
   "source": [
    "%pip install bitsandbytes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "664b3c7b-b5b7-4e70-a757-b4271bd64ad3",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024-03-08 09:56:20,889] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-03-08 09:56:29.647744: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\nTo enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n/local_disk0/.ephemeral_nfs/envs/pythonEnv-68e38c56-5d3d-43ab-9e2c-fc6ca917bc26/lib/python3.10/site-packages/bitsandbytes/cextension.py:31: UserWarning: The installed version of bitsandbytes was compiled without GPU support. 8-bit optimizers, 8-bit multiplication, and GPU quantization are unavailable.\n  warn(\"The installed version of bitsandbytes was compiled without GPU support. \"\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/local_disk0/.ephemeral_nfs/envs/pythonEnv-68e38c56-5d3d-43ab-9e2c-fc6ca917bc26/lib/python3.10/site-packages/bitsandbytes/libbitsandbytes_cpu.so: undefined symbol: cadam32bit_grad_fp32\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "# from accelerate import Accelerator, DeepSpeedPlugin\n",
    "# from accelerate import DistributedDataParallelKwargs\n",
    "from torch import optim\n",
    "from torch.optim import lr_scheduler\n",
    "\n",
    "from data_provider.m4 import M4Meta\n",
    "from models import Autoformer, DLinear, TimeLLM\n",
    "\n",
    "from data_provider.data_factory import data_provider\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas\n",
    "\n",
    "from utils.losses import smape_loss\n",
    "from utils.m4_summary import M4Summary\n",
    "import os\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "# tokenizer = AutoTokenizer.from_pretrained(\"luodian/llama-7b-hf\")\n",
    "# model = AutoModelForCausalLM.from_pretrained(\"luodian/llama-7b-hf\")\n",
    "\n",
    "os.environ['CURL_CA_BUNDLE'] = ''\n",
    "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"max_split_size_mb:64\"\n",
    "\n",
    "from utils.tools import del_files, EarlyStopping, adjust_learning_rate, load_content, test\n",
    "\n",
    "class Args:\n",
    "    def __init__(self):\n",
    "        self.task_name= 'short_term_forecast'\n",
    "        self.is_training= 1\n",
    "        self.model_id= 'm4_Weekly'\n",
    "        self.model_comment= 'TimeLLM-M4'\n",
    "        self.model= 'TimeLLM'\n",
    "        self.seed= 0\n",
    "        self.data= 'm4'\n",
    "        self.root_path= './dataset/m4'\n",
    "        self.data_path= 'm4.csv'\n",
    "        self.features= 'M'\n",
    "        self.target= 'OT'\n",
    "        self.loader= 'modal'\n",
    "        self.freq= 'h'\n",
    "        self.checkpoints= './checkpoints/'\n",
    "        self.seq_len= 96\n",
    "        self.label_len= 48\n",
    "        self.pred_len= 96\n",
    "        self.seasonal_patterns= 'Weekly'\n",
    "        self.enc_in= 1\n",
    "        self.dec_in= 1\n",
    "        self.c_out= 1\n",
    "        self.d_model= 8\n",
    "        self.n_heads= 8\n",
    "        self.e_layers= 2\n",
    "        self.d_layers= 1\n",
    "        self.d_ff= 32\n",
    "        self.moving_avg= 25\n",
    "        self.factor= 1\n",
    "        self.dropout= 0.1\n",
    "        self.embed= 'timeF'\n",
    "        self.activation= 'gelu'\n",
    "        self.output_attention= True\n",
    "        self.patch_len= 1\n",
    "        self.stride= 1\n",
    "        self.prompt_domain= 0\n",
    "        self.num_workers= 10\n",
    "        self.itr= 1\n",
    "        self.train_epochs= 50\n",
    "        self.align_epochs= 10\n",
    "        self.batch_size= 32\n",
    "        self.eval_batch_size= 8\n",
    "        self.patience= 20\n",
    "        self.learning_rate= 0.001\n",
    "        self.des= 'test'\n",
    "        self.loss= 'SMAPE'\n",
    "        self.lradj= 'type1'\n",
    "        self.pct_start= 0.2\n",
    "        self.use_amp= False\n",
    "        self.llm_layers= 32\n",
    "        self.percent= 10\n",
    "\n",
    "args = Args()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "360daea5-8e78-46a6-882a-8b007e0ffd42",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from accelerate import Accelerator, DeepSpeedPlugin\n",
    "from accelerate import DistributedDataParallelKwargs\n",
    "ddp_kwargs = DistributedDataParallelKwargs(find_unused_parameters=True)\n",
    "deepspeed_plugin = DeepSpeedPlugin(hf_ds_config='./ds_config_zero2.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "726448c5-cccf-4364-be8d-382bc65cfcbc",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "accelerator = Accelerator(kwargs_handlers=[ddp_kwargs], deepspeed_plugin=deepspeed_plugin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4f00bee1-ad95-4988-88e2-584d352e7b70",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "local variable 'model' referenced before assignment\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n",
       "\u001B[0;31mRuntimeError\u001B[0m                              Traceback (most recent call last)\n",
       "File \u001B[0;32m<command-1345708696421536>, line 36\u001B[0m\n",
       "\u001B[1;32m     29\u001B[0m test_data, test_loader \u001B[38;5;241m=\u001B[39m data_provider(args, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mtest\u001B[39m\u001B[38;5;124m'\u001B[39m)\n",
       "\u001B[1;32m     31\u001B[0m \u001B[38;5;66;03m# if args.model == 'Autoformer':\u001B[39;00m\n",
       "\u001B[1;32m     32\u001B[0m \u001B[38;5;66;03m#     model = Autoformer.Model(args).float()\u001B[39;00m\n",
       "\u001B[1;32m     33\u001B[0m \u001B[38;5;66;03m# elif args.model == 'DLinear':\u001B[39;00m\n",
       "\u001B[1;32m     34\u001B[0m \u001B[38;5;66;03m#     model = DLinear.Model(args).float()\u001B[39;00m\n",
       "\u001B[1;32m     35\u001B[0m \u001B[38;5;66;03m# else:\u001B[39;00m\n",
       "\u001B[0;32m---> 36\u001B[0m model \u001B[38;5;241m=\u001B[39m TimeLLM\u001B[38;5;241m.\u001B[39mModel(args)\u001B[38;5;241m.\u001B[39mfloat()\n",
       "\u001B[1;32m     38\u001B[0m path \u001B[38;5;241m=\u001B[39m os\u001B[38;5;241m.\u001B[39mpath\u001B[38;5;241m.\u001B[39mjoin(args\u001B[38;5;241m.\u001B[39mcheckpoints,\n",
       "\u001B[1;32m     39\u001B[0m                     setting \u001B[38;5;241m+\u001B[39m \u001B[38;5;124m'\u001B[39m\u001B[38;5;124m-\u001B[39m\u001B[38;5;124m'\u001B[39m \u001B[38;5;241m+\u001B[39m args\u001B[38;5;241m.\u001B[39mmodel_comment)  \u001B[38;5;66;03m# unique checkpoint saving path\u001B[39;00m\n",
       "\u001B[1;32m     40\u001B[0m args\u001B[38;5;241m.\u001B[39mcontent \u001B[38;5;241m=\u001B[39m load_content(args)\n",
       "\n",
       "File \u001B[0;32m/Workspace/Users/kanishthasiyaram15@gmail.com/Time-LLM/models/TimeLLM.py:77\u001B[0m, in \u001B[0;36mModel.__init__\u001B[0;34m(self, configs, patch_len, stride)\u001B[0m\n",
       "\u001B[1;32m     75\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mllama_config\u001B[38;5;241m.\u001B[39moutput_attentions \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mTrue\u001B[39;00m\n",
       "\u001B[1;32m     76\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mllama_config\u001B[38;5;241m.\u001B[39moutput_hidden_states \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mTrue\u001B[39;00m\n",
       "\u001B[0;32m---> 77\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mllama \u001B[38;5;241m=\u001B[39m \u001B[43mLlamaModel\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfrom_pretrained\u001B[49m\u001B[43m(\u001B[49m\n",
       "\u001B[1;32m     78\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;66;43;03m# \"/mnt/alps/modelhub/pretrained_model/LLaMA/7B_hf/\",\u001B[39;49;00m\n",
       "\u001B[1;32m     79\u001B[0m \u001B[43m    \u001B[49m\u001B[43mconfig_llama\u001B[49m\u001B[43m,\u001B[49m\n",
       "\u001B[1;32m     80\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;66;43;03m# 'huggyllama/llama-7b',\u001B[39;49;00m\n",
       "\u001B[1;32m     81\u001B[0m \u001B[43m    \u001B[49m\u001B[43mtrust_remote_code\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m,\u001B[49m\n",
       "\u001B[1;32m     82\u001B[0m \u001B[43m    \u001B[49m\u001B[43mlocal_files_only\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m,\u001B[49m\n",
       "\u001B[1;32m     83\u001B[0m \u001B[43m    \u001B[49m\u001B[43mconfig\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mllama_config\u001B[49m\u001B[43m,\u001B[49m\n",
       "\u001B[1;32m     84\u001B[0m \u001B[43m    \u001B[49m\u001B[43mload_in_4bit\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\n",
       "\u001B[1;32m     85\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n",
       "\u001B[1;32m     87\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtokenizer \u001B[38;5;241m=\u001B[39m LlamaTokenizer\u001B[38;5;241m.\u001B[39mfrom_pretrained(\n",
       "\u001B[1;32m     88\u001B[0m     \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m/mnt/alps/modelhub/pretrained_model/LLaMA/7B_hf/tokenizer.model\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n",
       "\u001B[1;32m     89\u001B[0m     \u001B[38;5;66;03m# 'huggyllama/llama-7b',\u001B[39;00m\n",
       "\u001B[1;32m     90\u001B[0m     trust_remote_code\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m,\n",
       "\u001B[1;32m     91\u001B[0m     local_files_only\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m\n",
       "\u001B[1;32m     92\u001B[0m )\n",
       "\u001B[1;32m     94\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtokenizer\u001B[38;5;241m.\u001B[39meos_token:\n",
       "\n",
       "File \u001B[0;32m/databricks/python_shell/dbruntime/huggingface_patches/transformers.py:21\u001B[0m, in \u001B[0;36m_create_patch_function.<locals>.patched_from_pretrained\u001B[0;34m(cls, *args, **kwargs)\u001B[0m\n",
       "\u001B[1;32m     19\u001B[0m call_succeeded \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mFalse\u001B[39;00m\n",
       "\u001B[1;32m     20\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n",
       "\u001B[0;32m---> 21\u001B[0m     model \u001B[38;5;241m=\u001B[39m \u001B[43moriginal_method\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[38;5;18;43m__func__\u001B[39;49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mcls\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
       "\u001B[1;32m     22\u001B[0m     call_succeeded \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mTrue\u001B[39;00m\n",
       "\u001B[1;32m     23\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m model\n",
       "\n",
       "File \u001B[0;32m/local_disk0/.ephemeral_nfs/cluster_libraries/python/lib/python3.10/site-packages/transformers/modeling_utils.py:2297\u001B[0m, in \u001B[0;36mPreTrainedModel.from_pretrained\u001B[0;34m(cls, pretrained_model_name_or_path, config, cache_dir, ignore_mismatched_sizes, force_download, local_files_only, token, revision, use_safetensors, *model_args, **kwargs)\u001B[0m\n",
       "\u001B[1;32m   2295\u001B[0m     device_map \u001B[38;5;241m=\u001B[39m {\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m\"\u001B[39m: torch\u001B[38;5;241m.\u001B[39mcuda\u001B[38;5;241m.\u001B[39mcurrent_device()}\n",
       "\u001B[1;32m   2296\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n",
       "\u001B[0;32m-> 2297\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mRuntimeError\u001B[39;00m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mNo GPU found. A GPU is needed for quantization.\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
       "\u001B[1;32m   2298\u001B[0m logger\u001B[38;5;241m.\u001B[39minfo(\n",
       "\u001B[1;32m   2299\u001B[0m     \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mThe device_map was not initialized.\u001B[39m\u001B[38;5;124m\"\u001B[39m\n",
       "\u001B[1;32m   2300\u001B[0m     \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mSetting device_map to \u001B[39m\u001B[38;5;124m{\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m:torch.cuda.current_device()}.\u001B[39m\u001B[38;5;124m\"\u001B[39m\n",
       "\u001B[1;32m   2301\u001B[0m     \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mIf you want to use the model for inference, please set device_map =\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mauto\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m \u001B[39m\u001B[38;5;124m\"\u001B[39m\n",
       "\u001B[1;32m   2302\u001B[0m )\n",
       "\u001B[1;32m   2303\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m low_cpu_mem_usage \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n",
       "\n",
       "\u001B[0;31mRuntimeError\u001B[0m: No GPU found. A GPU is needed for quantization."
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "datasetInfos": [],
       "jupyterProps": {
        "ename": "RuntimeError",
        "evalue": "No GPU found. A GPU is needed for quantization."
       },
       "metadata": {
        "errorSummary": "<span class='ansi-red-fg'>RuntimeError</span>: No GPU found. A GPU is needed for quantization."
       },
       "removedWidgets": [],
       "sqlProps": null,
       "stackFrames": [
        "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
        "\u001B[0;31mRuntimeError\u001B[0m                              Traceback (most recent call last)",
        "File \u001B[0;32m<command-1345708696421536>, line 36\u001B[0m\n\u001B[1;32m     29\u001B[0m test_data, test_loader \u001B[38;5;241m=\u001B[39m data_provider(args, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mtest\u001B[39m\u001B[38;5;124m'\u001B[39m)\n\u001B[1;32m     31\u001B[0m \u001B[38;5;66;03m# if args.model == 'Autoformer':\u001B[39;00m\n\u001B[1;32m     32\u001B[0m \u001B[38;5;66;03m#     model = Autoformer.Model(args).float()\u001B[39;00m\n\u001B[1;32m     33\u001B[0m \u001B[38;5;66;03m# elif args.model == 'DLinear':\u001B[39;00m\n\u001B[1;32m     34\u001B[0m \u001B[38;5;66;03m#     model = DLinear.Model(args).float()\u001B[39;00m\n\u001B[1;32m     35\u001B[0m \u001B[38;5;66;03m# else:\u001B[39;00m\n\u001B[0;32m---> 36\u001B[0m model \u001B[38;5;241m=\u001B[39m TimeLLM\u001B[38;5;241m.\u001B[39mModel(args)\u001B[38;5;241m.\u001B[39mfloat()\n\u001B[1;32m     38\u001B[0m path \u001B[38;5;241m=\u001B[39m os\u001B[38;5;241m.\u001B[39mpath\u001B[38;5;241m.\u001B[39mjoin(args\u001B[38;5;241m.\u001B[39mcheckpoints,\n\u001B[1;32m     39\u001B[0m                     setting \u001B[38;5;241m+\u001B[39m \u001B[38;5;124m'\u001B[39m\u001B[38;5;124m-\u001B[39m\u001B[38;5;124m'\u001B[39m \u001B[38;5;241m+\u001B[39m args\u001B[38;5;241m.\u001B[39mmodel_comment)  \u001B[38;5;66;03m# unique checkpoint saving path\u001B[39;00m\n\u001B[1;32m     40\u001B[0m args\u001B[38;5;241m.\u001B[39mcontent \u001B[38;5;241m=\u001B[39m load_content(args)\n",
        "File \u001B[0;32m/Workspace/Users/kanishthasiyaram15@gmail.com/Time-LLM/models/TimeLLM.py:77\u001B[0m, in \u001B[0;36mModel.__init__\u001B[0;34m(self, configs, patch_len, stride)\u001B[0m\n\u001B[1;32m     75\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mllama_config\u001B[38;5;241m.\u001B[39moutput_attentions \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mTrue\u001B[39;00m\n\u001B[1;32m     76\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mllama_config\u001B[38;5;241m.\u001B[39moutput_hidden_states \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mTrue\u001B[39;00m\n\u001B[0;32m---> 77\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mllama \u001B[38;5;241m=\u001B[39m \u001B[43mLlamaModel\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfrom_pretrained\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m     78\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;66;43;03m# \"/mnt/alps/modelhub/pretrained_model/LLaMA/7B_hf/\",\u001B[39;49;00m\n\u001B[1;32m     79\u001B[0m \u001B[43m    \u001B[49m\u001B[43mconfig_llama\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m     80\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;66;43;03m# 'huggyllama/llama-7b',\u001B[39;49;00m\n\u001B[1;32m     81\u001B[0m \u001B[43m    \u001B[49m\u001B[43mtrust_remote_code\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[1;32m     82\u001B[0m \u001B[43m    \u001B[49m\u001B[43mlocal_files_only\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[1;32m     83\u001B[0m \u001B[43m    \u001B[49m\u001B[43mconfig\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mllama_config\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m     84\u001B[0m \u001B[43m    \u001B[49m\u001B[43mload_in_4bit\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\n\u001B[1;32m     85\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     87\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtokenizer \u001B[38;5;241m=\u001B[39m LlamaTokenizer\u001B[38;5;241m.\u001B[39mfrom_pretrained(\n\u001B[1;32m     88\u001B[0m     \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m/mnt/alps/modelhub/pretrained_model/LLaMA/7B_hf/tokenizer.model\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[1;32m     89\u001B[0m     \u001B[38;5;66;03m# 'huggyllama/llama-7b',\u001B[39;00m\n\u001B[1;32m     90\u001B[0m     trust_remote_code\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m,\n\u001B[1;32m     91\u001B[0m     local_files_only\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m\n\u001B[1;32m     92\u001B[0m )\n\u001B[1;32m     94\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtokenizer\u001B[38;5;241m.\u001B[39meos_token:\n",
        "File \u001B[0;32m/databricks/python_shell/dbruntime/huggingface_patches/transformers.py:21\u001B[0m, in \u001B[0;36m_create_patch_function.<locals>.patched_from_pretrained\u001B[0;34m(cls, *args, **kwargs)\u001B[0m\n\u001B[1;32m     19\u001B[0m call_succeeded \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mFalse\u001B[39;00m\n\u001B[1;32m     20\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m---> 21\u001B[0m     model \u001B[38;5;241m=\u001B[39m \u001B[43moriginal_method\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[38;5;18;43m__func__\u001B[39;49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mcls\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     22\u001B[0m     call_succeeded \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mTrue\u001B[39;00m\n\u001B[1;32m     23\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m model\n",
        "File \u001B[0;32m/local_disk0/.ephemeral_nfs/cluster_libraries/python/lib/python3.10/site-packages/transformers/modeling_utils.py:2297\u001B[0m, in \u001B[0;36mPreTrainedModel.from_pretrained\u001B[0;34m(cls, pretrained_model_name_or_path, config, cache_dir, ignore_mismatched_sizes, force_download, local_files_only, token, revision, use_safetensors, *model_args, **kwargs)\u001B[0m\n\u001B[1;32m   2295\u001B[0m     device_map \u001B[38;5;241m=\u001B[39m {\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m\"\u001B[39m: torch\u001B[38;5;241m.\u001B[39mcuda\u001B[38;5;241m.\u001B[39mcurrent_device()}\n\u001B[1;32m   2296\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m-> 2297\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mRuntimeError\u001B[39;00m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mNo GPU found. A GPU is needed for quantization.\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m   2298\u001B[0m logger\u001B[38;5;241m.\u001B[39minfo(\n\u001B[1;32m   2299\u001B[0m     \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mThe device_map was not initialized.\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m   2300\u001B[0m     \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mSetting device_map to \u001B[39m\u001B[38;5;124m{\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m:torch.cuda.current_device()}.\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m   2301\u001B[0m     \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mIf you want to use the model for inference, please set device_map =\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mauto\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m   2302\u001B[0m )\n\u001B[1;32m   2303\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m low_cpu_mem_usage \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n",
        "\u001B[0;31mRuntimeError\u001B[0m: No GPU found. A GPU is needed for quantization."
       ],
       "type": "baseError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "for ii in range(args.itr):\n",
    "    # setting record of experiments\n",
    "    setting = '{}_{}_{}_{}_ft{}_sl{}_ll{}_pl{}_dm{}_nh{}_el{}_dl{}_df{}_fc{}_eb{}_{}_{}'.format(\n",
    "        args.task_name,\n",
    "        args.model_id,\n",
    "        args.model,\n",
    "        args.data,\n",
    "        args.features,\n",
    "        args.seq_len,\n",
    "        args.label_len,\n",
    "        args.pred_len,\n",
    "        args.d_model,\n",
    "        args.n_heads,\n",
    "        args.e_layers,\n",
    "        args.d_layers,\n",
    "        args.d_ff,\n",
    "        args.factor,\n",
    "        args.embed,\n",
    "        args.des, ii)\n",
    "\n",
    "    if args.data == 'm4':\n",
    "        args.pred_len = M4Meta.horizons_map[args.seasonal_patterns]  # Up to M4 config\n",
    "        args.seq_len = 2 * args.pred_len\n",
    "        args.label_len = args.pred_len\n",
    "        args.frequency_map = M4Meta.frequency_map[args.seasonal_patterns]\n",
    "\n",
    "    train_data, train_loader = data_provider(args, 'train')\n",
    "    vali_data, vali_loader = data_provider(args, 'val')\n",
    "    test_data, test_loader = data_provider(args, 'test')\n",
    "\n",
    "    # if args.model == 'Autoformer':\n",
    "    #     model = Autoformer.Model(args).float()\n",
    "    # elif args.model == 'DLinear':\n",
    "    #     model = DLinear.Model(args).float()\n",
    "    # else:\n",
    "    model = TimeLLM.Model(args).float()\n",
    "\n",
    "    path = os.path.join(args.checkpoints,\n",
    "                        setting + '-' + args.model_comment)  # unique checkpoint saving path\n",
    "    args.content = load_content(args)\n",
    "    if not os.path.exists(path) and accelerator.is_local_main_process:\n",
    "        os.makedirs(path)\n",
    "\n",
    "    time_now = time.time()\n",
    "\n",
    "    train_steps = len(train_loader)\n",
    "    early_stopping = EarlyStopping(accelerator=accelerator, patience=args.patience, verbose=True)\n",
    "\n",
    "    model_optim = optim.Adam(model.parameters(), lr=args.learning_rate)\n",
    "\n",
    "    if args.lradj == 'COS':\n",
    "        scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(model_optim, T_max=20, eta_min=1e-8)\n",
    "    else:\n",
    "        scheduler = lr_scheduler.OneCycleLR(optimizer=model_optim,\n",
    "                                            steps_per_epoch=train_steps,\n",
    "                                            pct_start=args.pct_start,\n",
    "                                            epochs=args.train_epochs,\n",
    "                                            max_lr=args.learning_rate)\n",
    "\n",
    "    criterion = smape_loss()\n",
    "\n",
    "    train_loader, vali_loader, model, model_optim, scheduler = accelerator.prepare(\n",
    "        train_loader, vali_loader, model, model_optim, scheduler)\n",
    "\n",
    "    for epoch in range(args.train_epochs):\n",
    "        iter_count = 0\n",
    "        train_loss = []\n",
    "\n",
    "        model.train()\n",
    "        epoch_time = time.time()\n",
    "\n",
    "        for i, (batch_x, batch_y, batch_x_mark, batch_y_mark) in enumerate(train_loader):\n",
    "            iter_count += 1\n",
    "            model_optim.zero_grad()\n",
    "            batch_x = batch_x.float().to(accelerator.device)\n",
    "\n",
    "            batch_y = batch_y.float().to(accelerator.device)\n",
    "            batch_y_mark = batch_y_mark.float().to(accelerator.device)\n",
    "\n",
    "            # decoder input\n",
    "            dec_inp = torch.zeros_like(batch_y[:, -args.pred_len:, :]).float().to(accelerator.device)\n",
    "            dec_inp = torch.cat([batch_y[:, :args.label_len, :], dec_inp], dim=1).float().to(\n",
    "                accelerator.device)\n",
    "\n",
    "            outputs = model(batch_x, None, dec_inp, None)\n",
    "\n",
    "            f_dim = -1 if args.features == 'MS' else 0\n",
    "            outputs = outputs[:, -args.pred_len:, f_dim:]\n",
    "            batch_y = batch_y[:, -args.pred_len:, f_dim:]\n",
    "\n",
    "            batch_y_mark = batch_y_mark[:, -args.pred_len:, f_dim:]\n",
    "            loss = criterion(batch_x, args.frequency_map, outputs, batch_y, batch_y_mark)\n",
    "\n",
    "            train_loss.append(loss.item())\n",
    "\n",
    "            if (i + 1) % 100 == 0:\n",
    "                accelerator.print(\n",
    "                    \"\\titers: {0}, epoch: {1} | loss: {2:.7f}\".format(i + 1, epoch + 1, loss.item())\n",
    "                )\n",
    "                speed = (time.time() - time_now) / iter_count\n",
    "                left_time = speed * ((args.train_epochs - epoch) * train_steps - i)\n",
    "                accelerator.print('\\tspeed: {:.4f}s/iter; left time: {:.4f}s'.format(speed, left_time))\n",
    "                iter_count = 0\n",
    "                time_now = time.time()\n",
    "\n",
    "            accelerator.backward(loss)\n",
    "            model_optim.step()\n",
    "\n",
    "            if args.lradj == 'TST':\n",
    "                adjust_learning_rate(accelerator, model_optim, scheduler, epoch + 1, args, printout=False)\n",
    "                scheduler.step()\n",
    "\n",
    "        accelerator.print(\"Epoch: {} cost time: {}\".format(epoch + 1, time.time() - epoch_time))\n",
    "        train_loss = np.average(train_loss)\n",
    "        vali_loss = test(args, accelerator, model, train_loader, vali_loader, criterion)\n",
    "        test_loss = vali_loss\n",
    "        accelerator.print(\n",
    "            \"Epoch: {0}, Steps: {1} | Train Loss: {2:.7f} Vali Loss: {3:.7f} Test Loss: {4:.7f}\".format(\n",
    "                epoch + 1, train_steps, train_loss, vali_loss, test_loss))\n",
    "        early_stopping(vali_loss, model, path)  # model saving\n",
    "        if early_stopping.early_stop:\n",
    "            accelerator.print(\"Early stopping\")\n",
    "            break\n",
    "\n",
    "        if args.lradj != 'TST':\n",
    "            adjust_learning_rate(accelerator, model_optim, scheduler, epoch + 1, args, printout=True)\n",
    "        else:\n",
    "            accelerator.print('Updating learning rate to {}'.format(scheduler.get_last_lr()[0]))\n",
    "\n",
    "    best_model_path = path + '/' + 'checkpoint'\n",
    "    accelerator.wait_for_everyone()\n",
    "    unwrapped_model = accelerator.unwrap_model(model)\n",
    "    torch.cuda.synchronize()\n",
    "    torch.cuda.empty_cache()\n",
    "    unwrapped_model.load_state_dict(torch.load(best_model_path, map_location=lambda storage, loc: storage))\n",
    "\n",
    "    x, _ = train_loader.dataset.last_insample_window()\n",
    "    y = test_loader.dataset.timeseries\n",
    "    x = torch.tensor(x, dtype=torch.float32).to(accelerator.device)\n",
    "    x = x.unsqueeze(-1)\n",
    "\n",
    "    model.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        B, _, C = x.shape\n",
    "        dec_inp = torch.zeros((B, args.pred_len, C)).float().to(accelerator.device)\n",
    "        dec_inp = torch.cat([x[:, -args.label_len:, :], dec_inp], dim=1)\n",
    "        outputs = torch.zeros((B, args.pred_len, C)).float().to(accelerator.device)\n",
    "        id_list = np.arange(0, B, args.eval_batch_size)\n",
    "        id_list = np.append(id_list, B)\n",
    "        for i in range(len(id_list) - 1):\n",
    "            outputs[id_list[i]:id_list[i + 1], :, :] = model(\n",
    "                x[id_list[i]:id_list[i + 1]],\n",
    "                None,\n",
    "                dec_inp[id_list[i]:id_list[i + 1]],\n",
    "                None\n",
    "            )\n",
    "        accelerator.wait_for_everyone()\n",
    "        f_dim = -1 if args.features == 'MS' else 0\n",
    "        outputs = outputs[:, -args.pred_len:, f_dim:]\n",
    "        outputs = outputs.detach().cpu().numpy()\n",
    "\n",
    "        preds = outputs\n",
    "        trues = y\n",
    "        x = x.detach().cpu().numpy()\n",
    "\n",
    "    accelerator.print('test shape:', preds.shape)\n",
    "\n",
    "    folder_path = './m4_results/' + args.model + '-' + args.model_comment + '/'\n",
    "    if not os.path.exists(folder_path) and accelerator.is_local_main_process:\n",
    "        os.makedirs(folder_path)\n",
    "\n",
    "    if accelerator.is_local_main_process:\n",
    "        forecasts_df = pandas.DataFrame(preds[:, :, 0], columns=[f'V{i + 1}' for i in range(args.pred_len)])\n",
    "        forecasts_df.index = test_loader.dataset.ids[:preds.shape[0]]\n",
    "        forecasts_df.index.name = 'id'\n",
    "        forecasts_df.set_index(forecasts_df.columns[0], inplace=True)\n",
    "        forecasts_df.to_csv(folder_path + args.seasonal_patterns + '_forecast.csv')\n",
    "\n",
    "        # calculate metrics\n",
    "        accelerator.print(args.model)\n",
    "        file_path = folder_path\n",
    "        if 'Weekly_forecast.csv' in os.listdir(file_path) :\n",
    "                # and 'Monthly_forecast.csv' in os.listdir(file_path) \\\n",
    "                # and 'Yearly_forecast.csv' in os.listdir(file_path) \\\n",
    "                # and 'Daily_forecast.csv' in os.listdir(file_path) \\\n",
    "                # and 'Hourly_forecast.csv' in os.listdir(file_path) \\\n",
    "                # and 'Quarterly_forecast.csv' in os.listdir(file_path):\n",
    "            m4_summary = M4Summary(file_path, args.root_path)\n",
    "            # m4_forecast.set_index(m4_winner_forecast.columns[0], inplace=True)\n",
    "            smape_results, owa_results, mape, mase = m4_summary.evaluate()\n",
    "            accelerator.print('smape:', smape_results)\n",
    "            accelerator.print('mape:', mape)\n",
    "            accelerator.print('mase:', mase)\n",
    "            accelerator.print('owa:', owa_results)\n",
    "        else:\n",
    "            accelerator.print('After all 6 tasks are finished, you can calculate the averaged performance')\n",
    "\n",
    "accelerator.wait_for_everyone()\n",
    "if accelerator.is_local_main_process:\n",
    "    path = './checkpoints'  # unique checkpoint saving path\n",
    "    del_files(path)  # delete checkpoint files\n",
    "    accelerator.print('success delete checkpoints')\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "run_m4",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
